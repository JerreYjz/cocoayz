{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e6c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import camb\n",
    "import scipy.linalg\n",
    "from camb import model, initialpower\n",
    "from camb.dark_energy import DarkEnergyPPF, DarkEnergyFluid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34aa5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SET UP CMB POWER SPECTRA RANGE #####\n",
    "\n",
    "camb_ell_min          = 2\n",
    "camb_ell_max          = 5000\n",
    "camb_ell_range        = camb_ell_max - camb_ell_min\n",
    "\n",
    "##### PICK DEVICE ON WHICH THE MODEL WILL BE TRAINED ON. WE RECOMMEND USING GPU FOR TRAINING #####\n",
    "device                = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#CUDA for GPU\n",
    "#CPU for CPU\n",
    "#GPU is generally recommended for higher speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c936dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOAD UP MEAN AND STD FOR INPUT AND OUTPUT #####\n",
    "extrainfo=np.load(\"extra/extrainfo_plk_tt_first_T256.npy\",allow_pickle=True)\n",
    "X_mean=torch.Tensor(extrainfo.item()['X_mean'])#.to(device)\n",
    "X_std=torch.Tensor(extrainfo.item()['X_std'])#.to(device)\n",
    "Y_mean=torch.Tensor(extrainfo.item()['Y_mean']).to(device)\n",
    "Y_std=torch.Tensor(extrainfo.item()['Y_std']).to(device)\n",
    "\n",
    "##### LOAD UP COV MAT #####\n",
    "covinv=np.load('/home/grads/data/yijie/mltrial/extra/cosvarinvTT.npy',allow_pickle=True)[:camb_ell_range,:camb_ell_range]\n",
    "covinv=torch.Tensor(covinv).to(device) #This is inverse of the Covariance Matrix\n",
    "\n",
    "\n",
    "#load in data\n",
    "train_samples=np.load('parametersamples/cos_pkc_T256_firsttest.npy',allow_pickle=True)\n",
    "\n",
    "validation_samples=np.load('parametersamples/cos_pkc_T128.npy',allow_pickle=True)[:10000]\n",
    "\n",
    "train_data_vectors=np.load('datavectors/Planck256/cos_pkc_T256_firsttest_TT.npy',allow_pickle=True,mmap_mode='r+')\n",
    "\n",
    "validation_data_vectors=np.load('datavectors/Planck128/cos_pkc_T128_TT_acc.npy',allow_pickle=True)[:10000,:4998]\n",
    "train_samples=torch.Tensor(train_samples)\n",
    "train_data_vectors=torch.Tensor(train_data_vectors)\n",
    "validation_samples=torch.Tensor(validation_samples)\n",
    "validation_data_vectors=torch.Tensor(validation_data_vectors)\n",
    "#specifying input and output dimension of our model\n",
    "input_size=len(train_samples[0])\n",
    "out_size=len(train_data_vectors[0])\n",
    "\n",
    "\n",
    "\n",
    "#normalizing samples and to mean 0, std 1\n",
    "\n",
    "X_train=(train_samples-X_mean)/X_std\n",
    "X_train[:,6:]=0 # we didn't vary the last 3 parameters: mnu, w, and wa in this test, so setting them to 0 automatically after normalization\n",
    "\n",
    "X_validation=(validation_samples-X_mean)/X_std\n",
    "X_validation[:,6:]=0 # we didn't vary the last 3 parameters: mnu, w, and wa in this test, so setting them to 0 automatically after normalization\n",
    "\n",
    "X_train=X_train.to(torch.float32)\n",
    "X_validation=X_validation.to(torch.float32)\n",
    "\n",
    "X_mean=X_mean.to(device)\n",
    "X_std=X_std.to(device)\n",
    "\n",
    "#load the data to batches. Do not send those to device yet to save space\n",
    "batch_size = 512\n",
    "\n",
    "trainset    = TensorDataset(X_train, train_data_vectors)\n",
    "validset    = TensorDataset(X_validation,validation_data_vectors)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=1)\n",
    "validloader = DataLoader(validset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "549fbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### START DEFINING THE MACHINE LEARNING MODULES #####\n",
    "\n",
    "class Supact(nn.Module):\n",
    "    # Dynamical activation function, returns:\n",
    "    # h(x)=(gamma+(1+exp(-beta*x))^(-1)*(1-gamma))*x\n",
    "    # gamma and beta are trainable parameters.\n",
    "    # We chose the initial value for gamma to be all 1, and beta to be all 0\n",
    "    def __init__(self, in_size):\n",
    "        # need to specify the input size for this activation function\n",
    "        super(Supact, self).__init__()\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.ones(in_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(in_size))\n",
    "        self.m = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        inv = self.m(torch.mul(self.beta,x))\n",
    "        fac = 1-self.gamma\n",
    "        mult = self.gamma + torch.mul(inv,fac)\n",
    "        return torch.mul(mult,x)\n",
    "\n",
    "class Affine(nn.Module):\n",
    "    def __init__(self):\n",
    "        # This function is designed for the Neuro-network to learn how to normalize the data between\n",
    "        # layers. we will initiate gains and bias both at 1 \n",
    "        \n",
    "        super(Affine, self).__init__()\n",
    "\n",
    "        \n",
    "        self.gain = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return x * self.gain + self.bias\n",
    "\n",
    "class Better_Attention(nn.Module):\n",
    "    # This is the Dot-Product Attention Layer. Returns:\n",
    "    # Attention(Q,K,V)=Softmax(QK^T/sqrt{d_K})V\n",
    "    def __init__(self, in_size ,n_partitions):\n",
    "        super(Better_Attention, self).__init__()\n",
    "\n",
    "        self.embed_dim    = in_size//n_partitions\n",
    "        self.WQ           = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.WK           = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.WV           = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "        self.act          = nn.Softmax(dim=1) #NOT along the batch direction, apply to each vector.\n",
    "        self.scale        = np.sqrt(self.embed_dim)\n",
    "        self.n_partitions = n_partitions # n_partions or n_channels are synonyms \n",
    "        self.norm         = torch.nn.LayerNorm(in_size) # layer norm has geometric order (https://lessw.medium.com/what-layernorm-really-does-for-attention-in-transformers-4901ea6d890e)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm    = self.norm(x)\n",
    "        batch_size = x.shape[0]\n",
    "        _x = x_norm.reshape(batch_size,self.n_partitions,self.embed_dim) # put into channels\n",
    "\n",
    "        Q = self.WQ(_x) # query with q_i as rows\n",
    "        K = self.WK(_x) # key   with k_i as rows\n",
    "        V = self.WV(_x) # value with v_i as rows\n",
    "\n",
    "        dot_product = torch.bmm(Q,K.transpose(1, 2).contiguous())\n",
    "        normed_mat  = self.act(dot_product/self.scale)\n",
    "        prod        = torch.bmm(normed_mat,V)\n",
    "\n",
    "        out = torch.reshape(prod,(batch_size,-1))+x # reshape back to vector\n",
    "\n",
    "        return out\n",
    "\n",
    "class Better_Transformer(nn.Module):\n",
    "    # This is the Transformer Block. The input data vectors are splitted into channels and then pass through\n",
    "    # DIFFERENT MLP Blocks(Note that this is different from the original transformer design).\n",
    "    # Adding residual at the end before output.\n",
    "    def __init__(self, in_size, n_partitions):\n",
    "        super(Better_Transformer, self).__init__()  \n",
    "    \n",
    "        # get/set up hyperparams\n",
    "        self.int_dim      = in_size//n_partitions \n",
    "        self.n_partitions = n_partitions\n",
    "        self.act          = Supact(in_size)#nn.Tanh()#nn.ReLU()#\n",
    "        self.norm         = Affine()\n",
    "\n",
    "        # set up weight matrices and bias vectors\n",
    "        weights = torch.zeros((n_partitions,self.int_dim,self.int_dim))\n",
    "        self.weights = nn.Parameter(weights) # turn the weights tensor into trainable weights\n",
    "        bias = torch.Tensor(in_size)\n",
    "        self.bias = nn.Parameter(bias) # turn bias tensor into trainable weights\n",
    "\n",
    "        # initialize weights and biases\n",
    "        # this process follows the standard from the nn.Linear module (https://auro-227.medium.com/writing-a-custom-layer-in-pytorch-14ab6ac94b77)\n",
    "        nn.init.kaiming_uniform_(self.weights, a=np.sqrt(5)) # matrix weights init \n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights) # fan_in in the input size, fan out is the output size but it is not use here\n",
    "        bound = 1 / np.sqrt(fan_in) \n",
    "        nn.init.uniform_(self.bias, -bound, bound) # bias weights init\n",
    "\n",
    "    def forward(self,x):\n",
    "        mat = torch.block_diag(*self.weights) \n",
    "        x_norm = self.norm(x)\n",
    "        o = self.act(torch.matmul(x_norm,mat)+self.bias)\n",
    "        return o+x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    #Residual Dense Block. The input goes through two dense layers and then add input to output(Residual part).\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(ResBlock, self).__init__()\n",
    "        #get/set up parameters\n",
    "        if in_size != out_size: \n",
    "            self.skip = nn.Linear(in_size, out_size, bias=False)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_size, out_size)\n",
    "        self.layer2 = nn.Linear(out_size, out_size)\n",
    "\n",
    "        self.norm1 = Affine()\n",
    "        self.norm2 = Affine()\n",
    "\n",
    "        self.act1 = Supact(in_size)#nn.Tanh()#nn.ReLU()#\n",
    "        self.act2 = Supact(in_size)#nn.Tanh()#nn.ReLU()#\n",
    "\n",
    "    def forward(self, x):\n",
    "        xskip = self.skip(x)\n",
    "\n",
    "        o1 = self.act1(self.layer1(self.norm1(x)))\n",
    "        o2 = self.act2(self.layer2(self.norm2(o1))) + xskip\n",
    "\n",
    "        return o2\n",
    "\n",
    "\n",
    "class ResMLP(nn.Module):\n",
    "    # ResMLP architecture:\n",
    "    # Input Layer\n",
    "    # N_block amount of ResBlocks\n",
    "    # Output Layer\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, int_dim, N_block):\n",
    "\n",
    "        super(ResMLP, self).__init__()\n",
    "\n",
    "        modules=[]\n",
    "\n",
    "        # Def: we will set the internal dimension as multiple of 128 (reason: just simplicity)\n",
    "        int_dim = int_dim * 128\n",
    "\n",
    "        # Def: we will only change the dimension of the datavector using linear transformations  \n",
    "        modules.append(nn.Linear(input_dim, int_dim))\n",
    "        \n",
    "        # Def: by design, a pure block has the input and output dimension to be the same\n",
    "        for n in range(N_block):\n",
    "            # One full ResBlock contains 1 ResLayer + 1 Activation Function \n",
    "            modules.append(ResBlock(int_dim, int_dim))\n",
    "            modules.append(Supact(int_dim))\n",
    "        \n",
    "        # Def: the transformation from the internal dimension to the output dimension of the\n",
    "        #      data vector we intend to emulate\n",
    "        \n",
    "        modules.append(nn.Linear(int_dim, output_dim))\n",
    "        modules.append(Affine())\n",
    "        # NN.SEQUENTIAL is a PYTHORCH function DEFINED AT: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "        # This function stacks up layers in the modules-list in sequence to create the whole model\n",
    "        self.resmlp =nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x is a cosmological parameter set you feed in the model\n",
    "        out = self.resmlp(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TRF(nn.Module):\n",
    "    # Transformer architecture: \n",
    "    # Input Layer\n",
    "    # 3 ResBlocks\n",
    "    # 1 Attention Layer+Transformer Block\n",
    "    # output layer\n",
    "    '''\n",
    "    The Users are welcomed to modify the number of layers or width of blocks in this architecture\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, int_dim, int_trf, N_channels):\n",
    "\n",
    "        super(TRF, self).__init__()\n",
    "\n",
    "        modules=[]\n",
    "\n",
    "        int_dim = int_dim * 128\n",
    "        \n",
    "        n_channels = N_channels\n",
    "        int_dim_trf = int_trf\n",
    "        modules.append(nn.Linear(input_dim, int_dim))\n",
    "        modules.append(ResBlock(int_dim, int_dim))\n",
    "        modules.append(Supact(int_dim))\n",
    "        modules.append(ResBlock(int_dim, int_dim))\n",
    "        modules.append(Supact(int_dim))\n",
    "        modules.append(ResBlock(int_dim, int_dim))\n",
    "        modules.append(Supact(int_dim))\n",
    "        modules.append(nn.Linear(int_dim, int_dim_trf))\n",
    "        modules.append(Supact(int_dim_trf))\n",
    "        modules.append(Better_Attention(int_dim_trf, n_channels))\n",
    "        modules.append(Better_Transformer(int_dim_trf, n_channels))\n",
    "        modules.append(nn.Linear(int_dim_trf, output_dim))\n",
    "        modules.append(Affine())\n",
    "\n",
    "        self.trf =nn.Sequential(*modules)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #x is a cosmological parameter set you feed in the model\n",
    "        out = self.trf(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c88b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "intdim = 4    # internal dimension of the ResMLP blocks\n",
    "int_trf = 5120# internal dimension of the Transformer block\n",
    "nc=32         # number of channels we pick\n",
    "\n",
    "#Set up the model and optimizer\n",
    "model = TRF(input_dim=input_size,output_dim=out_size,int_dim=intdim, int_trf=int_trf,N_channels=nc)\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd160db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train = []\n",
    "losses_vali = []\n",
    "losses_train_med = []\n",
    "losses_vali_med = []\n",
    "\n",
    "# Setting up the learning rate scheduler\n",
    "reduce_lr = True#reducing learning rate on plateau\n",
    "if reduce_lr==True:\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.5,patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TRAINING PROCESS #####\n",
    "# Here we use the loss function of L=\\sqrt(1+2\\chi^2). The users should test out\n",
    "# their own best Loss functions and modify the code correspondingly.\n",
    "\n",
    "\n",
    "n_epoch = 700\n",
    "for n in range(n_epoch):\n",
    "    losses=[]\n",
    "    for i, data in enumerate(trainloader):\n",
    "        model.train()         # turn on the model to training mode\n",
    "\n",
    "        X = data[0].to(device)# send to device one by one\n",
    "            \n",
    "        Y_batch = data[1].to(device)# send to device one by one\n",
    "        Y_pred  = model(X).to(device)# model output is normalized C_ell*e^(2tau_re)/As\n",
    "        #scale the output back to C_ell unit\n",
    "        As = torch.exp(X[:,5]*X_std[0,5]+X_mean[0,5])\n",
    "        exptau = torch.exp(2*X[:,3]*X_std[0,3]+2*X_mean[0,3])\n",
    "        Y_pred =  Y_pred*Y_std+Y_mean             # un-normalize to C_ell/A_s*exp(2tau)\n",
    "        Y_pred = Y_pred *As[:,None]/exptau[:,None]# put back A_s/exp(2tau)\n",
    "        diff = Y_pred - Y_batch\n",
    "\n",
    "        loss1 = torch.diag(diff @ covinv @ torch.t(diff)) # computing chi2\n",
    "        loss1 = torch.sqrt(1+2*loss1)                     # computing loss in our definition\n",
    "        loss=torch.mean(loss1)\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses_train.append(np.mean(losses))# We take means since a loss function should return a single real number\n",
    "    losses_train_med.append(np.median(losses))\n",
    "\n",
    "    # cross-validation process\n",
    "    with torch.no_grad():\n",
    "        model.eval() # turn the model onto evaluating mode, so no training happens\n",
    "        \n",
    "        losses = []\n",
    "        for i, data in enumerate(validloader):\n",
    "            X_v       = data[0].to(device)\n",
    "            Y_v_batch = data[1].to(device)\n",
    "            Y_v_pred = model(X_v).to(device)\n",
    "            As=torch.exp(X_v[:,5]*X_std[0,5]+X_mean[0,5])\n",
    "            exptau=torch.exp(2*X_v[:,3]*X_std[0,3]+2*X_mean[0,3])\n",
    "            Y_v_pred_back = Y_v_pred*Y_std+Y_mean\n",
    "            Y_v_pred_back = Y_v_pred_back*As[:,None]/exptau[:,None]\n",
    "            v_diff = (Y_v_batch - Y_v_pred_back)\n",
    "            loss1 = torch.diag(v_diff @ covinv @ torch.t(v_diff))\n",
    "            loss1 = torch.sqrt(1+2*loss1)\n",
    "            loss_vali=torch.mean(loss1)\n",
    "            losses.append(loss_vali.cpu().detach().numpy())\n",
    "\n",
    "        losses_vali.append(np.mean(losses))\n",
    "        losses_vali_med.append(np.median(losses))\n",
    "\n",
    "        if reduce_lr == True:\n",
    "            print('Reduce LR on plateu: ',reduce_lr)\n",
    "            scheduler.step(losses_vali[n])\n",
    "\n",
    "    print('epoch {}, loss={}, validation loss={}, lr={}, wd={})'.format(\n",
    "                        n,\n",
    "                        losses_train[-1],\n",
    "                        losses_vali[-1],\n",
    "                        optimizer.param_groups[0]['lr'],\n",
    "                        optimizer.param_groups[0]['weight_decay']\n",
    "                        \n",
    "                    ))\n",
    "\n",
    "    # saving the models\n",
    "\n",
    "PATH = \"./trainedemu5000plktrf/chiTTAstauc\"+str(nc)\n",
    "torch.save(model.state_dict(), PATH+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807582b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
