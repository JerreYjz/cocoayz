{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import camb\n",
    "import scipy.linalg\n",
    "from camb import model, initialpower\n",
    "from camb.dark_energy import DarkEnergyPPF, DarkEnergyFluid\n",
    "from fast_transformers.attention import AttentionLayer\n",
    "from fast_transformers.attention import FullAttention\n",
    "from fast_transformers.masking import FullMask, TriangularCausalMask\n",
    "from fast_transformers.attention import FullAttention, LinearAttention, ReformerAttention, AFTFullAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aa5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the ell range\n",
    "camb_ell_min          = 2#30\n",
    "camb_ell_max          = 5000\n",
    "camb_ell_range        = camb_ell_max  - camb_ell_min \n",
    "\n",
    "#load test data\n",
    "testing_samples=np.load('parametersamples/cos_pkc_T128.npy',allow_pickle=True)[10000:]\n",
    "\n",
    "testing_data_vectors=np.load('datavectors/Planck128/cos_pkc_T128_TT_acc.npy',allow_pickle=True)[10000:,:4998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set device\n",
    "#I prefer using CPU for model usage after training\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#set the number of channels and internal dimension of the ResMLP blocks in the TRF\n",
    "nc=32\n",
    "intdim=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load normalization factors of the model, and model parameters\n",
    "\n",
    "PATH = \"./trainedemu5000plktrf/chiTTAstautrf150k256val128i5120evansqrtc\"+str(nc)\n",
    "extrainfo=np.load(\"extra/extrainfo_plk_tt_third_T256.npy\",allow_pickle=True)\n",
    "X_mean=torch.Tensor(extrainfo.item()['X_mean']).to(device)\n",
    "X_std=torch.Tensor(extrainfo.item()['X_std']).to(device)\n",
    "Y_mean=torch.Tensor(extrainfo.item()['Y_mean']).to(device)\n",
    "Y_std=torch.Tensor(extrainfo.item()['Y_std']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fbcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML modules\n",
    "class Supact(nn.Module):\n",
    "    # New activation function, returns:\n",
    "    # f(x)=(gamma+(1+exp(-beta*x))^(-1)*(1-gamma))*x\n",
    "    # gamma and beta are trainable parameters.\n",
    "    # I chose the initial value for gamma to be all 1, and beta to be all 0\n",
    "    def __init__(self, in_size):\n",
    "        super(Supact, self).__init__()\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.ones(in_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(in_size))\n",
    "        self.m = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        inv = self.m(torch.mul(self.beta,x))\n",
    "        fac = 1-self.gamma\n",
    "        mult = self.gamma + torch.mul(inv,fac)\n",
    "        return torch.mul(mult,x)\n",
    "\n",
    "class Affine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Affine, self).__init__()\n",
    "\n",
    "        # This function is designed for the Neuro-network to learn how to normalize the data between\n",
    "        # layers. we will initiate gains and bias both at 1 \n",
    "        self.gain = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return x * self.gain + self.bias\n",
    "\n",
    "class Better_Attention(nn.Module):\n",
    "    def __init__(self, in_size ,n_partitions):\n",
    "        super(Better_Attention, self).__init__()\n",
    "\n",
    "        self.embed_dim    = in_size//n_partitions\n",
    "        self.WQ           = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.WK           = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.WV           = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "        self.act          = nn.Softmax(dim=1) #NOT along the batch direction, apply to each vector.\n",
    "        self.scale        = np.sqrt(self.embed_dim)\n",
    "        self.n_partitions = n_partitions # n_partions or n_channels are synonyms \n",
    "        self.norm         = torch.nn.LayerNorm(in_size) # layer norm has geometric order (https://lessw.medium.com/what-layernorm-really-does-for-attention-in-transformers-4901ea6d890e)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm    = self.norm(x)\n",
    "        batch_size = x.shape[0]\n",
    "        _x = x_norm.reshape(batch_size,self.n_partitions,self.embed_dim) # put into channels\n",
    "\n",
    "        Q = self.WQ(_x) # query with q_i as rows\n",
    "        K = self.WK(_x) # key   with k_i as rows\n",
    "        V = self.WV(_x) # value with v_i as rows\n",
    "\n",
    "        dot_product = torch.bmm(Q,K.transpose(1, 2).contiguous())\n",
    "        normed_mat  = self.act(dot_product/self.scale)\n",
    "        prod        = torch.bmm(normed_mat,V)\n",
    "\n",
    "        #out = torch.cat(tuple([prod[:,i] for i in range(self.n_partitions)]),dim=1)+x\n",
    "        out = torch.reshape(prod,(batch_size,-1))+x # reshape back to vector\n",
    "\n",
    "        return out\n",
    "\n",
    "class Better_Transformer(nn.Module):\n",
    "    def __init__(self, in_size, n_partitions):\n",
    "        super(Better_Transformer, self).__init__()  \n",
    "    \n",
    "        # get/set up hyperparams\n",
    "        self.int_dim      = in_size//n_partitions \n",
    "        self.n_partitions = n_partitions\n",
    "        self.act          = Supact(in_size)#nn.Tanh()#nn.ReLU()#\n",
    "        self.norm         = Affine()#torch.nn.BatchNorm1d(in_size)\n",
    "\n",
    "        # set up weight matrices and bias vectors\n",
    "        weights = torch.zeros((n_partitions,self.int_dim,self.int_dim))\n",
    "        self.weights = nn.Parameter(weights) # turn the weights tensor into trainable weights\n",
    "        bias = torch.Tensor(in_size)\n",
    "        self.bias = nn.Parameter(bias) # turn bias tensor into trainable weights\n",
    "\n",
    "        # initialize weights and biases\n",
    "        # this process follows the standard from the nn.Linear module (https://auro-227.medium.com/writing-a-custom-layer-in-pytorch-14ab6ac94b77)\n",
    "        nn.init.kaiming_uniform_(self.weights, a=np.sqrt(5)) # matrix weights init \n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights) # fan_in in the input size, fan out is the output size but it is not use here\n",
    "        bound = 1 / np.sqrt(fan_in) \n",
    "        nn.init.uniform_(self.bias, -bound, bound) # bias weights init\n",
    "\n",
    "    def forward(self,x):\n",
    "        mat = torch.block_diag(*self.weights) # how can I do this on init rather than on each forward pass?\n",
    "        x_norm = self.norm(x)\n",
    "        #_x = x_norm.reshape(x_norm.shape[0],self.n_partitions,self.int_dim) # reshape into channels\n",
    "        o = self.act(torch.matmul(x_norm,mat)+self.bias)\n",
    "        return o+x\n",
    "\n",
    "class AttnLayer(nn.Module):\n",
    "    def __init__(self, in_size, n_partitions, n_heads, att):\n",
    "        super(AttnLayer, self).__init__()\n",
    "        \n",
    "        self.n_heads       = n_heads\n",
    "        self.n_partitions  = n_partitions\n",
    "        self.d_model       = in_size//n_partitions\n",
    "        \n",
    "        # Types of Attention I tested:\n",
    "        \n",
    "        # att = FullAttention()  <- Standard\n",
    "        # att = LinearAttention(int_dim_trf//n_channels)\n",
    "        # att = ReformerAttention()\n",
    "        # att = AFTFullAttention(max_sequence_length = n_channels, aft_parameterization = n_channels)\n",
    "        \n",
    "        # Details for the above functions can be found at:\n",
    "        # https://fast-transformers.github.io/api_docs/fast_transformers/attention/\n",
    "        \n",
    "        self.layer         = AttentionLayer(att, self.d_model, n_heads, self.d_model, self.d_model)\n",
    "        \n",
    "        self.WQ            = nn.Linear(self.d_model, self.d_model)\n",
    "        self.WK            = nn.Linear(self.d_model, self.d_model)\n",
    "        self.WV            = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "        self.attn_mask     = None\n",
    "        self.query_lengths = None\n",
    "        self.key_lengths   = None\n",
    "        \n",
    "        self.norm          = nn.LayerNorm(in_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm(x)\n",
    "        \n",
    "        B = x.shape[0]        # Batch Size\n",
    "        L = self.n_partitions # Sequence Length\n",
    "\n",
    "        \n",
    "        _x = x_norm.reshape(B, L, self.d_model)\n",
    "        \n",
    "        if self.attn_mask is None:\n",
    "            self.attn_mask     = FullMask(L, L, device=x.device) # Which keys can attend to which queries?\n",
    "            self.query_lengths = FullMask(B, L, device=x.device) # Query Lengths (= n_partitions for all our data)\n",
    "            self.key_lengths   = FullMask(B, L, device=x.device) # Key Lengths   (= n_partitions for all our data)\n",
    "        \n",
    "        queries = self.WQ(_x)\n",
    "        keys = self.WK(_x)\n",
    "        values = self.WV(_x)\n",
    "        \n",
    "        result = self.layer(queries, keys, values, self.attn_mask, self.query_lengths, self.key_lengths)\n",
    "        \n",
    "        out = torch.reshape(result, (B, -1)) + x\n",
    "        \n",
    "        return out\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        if in_size != out_size: \n",
    "            self.skip = nn.Linear(in_size, out_size, bias=False) # we don't consider this. remove?\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_size, out_size)\n",
    "        self.layer2 = nn.Linear(out_size, out_size)\n",
    "\n",
    "        self.norm1 = Affine()\n",
    "        self.norm2 = Affine()\n",
    "\n",
    "        self.act1 = Supact(in_size)#nn.Tanh()#nn.ReLU()#\n",
    "        self.act2 = Supact(in_size)#nn.Tanh()#nn.ReLU()#\n",
    "\n",
    "    def forward(self, x):\n",
    "        xskip = self.skip(x)\n",
    "\n",
    "        o1 = self.act1(self.layer1(self.norm1(x)))\n",
    "        o2 = self.act2(self.layer2(self.norm2(o1))) + xskip\n",
    "\n",
    "        return o2\n",
    "\n",
    "class ResMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, int_dim, N_layer):\n",
    "\n",
    "        super(ResMLP, self).__init__()\n",
    "\n",
    "        modules=[]\n",
    "\n",
    "        # Def: we will set the internal dimension as multiple of 128 (reason: just simplicity)\n",
    "        int_dim = int_dim * 128\n",
    "\n",
    "        # Def: we will only change the dimension of the datavector using linear transformations  \n",
    "        modules.append(nn.Linear(input_dim, int_dim))\n",
    "        \n",
    "        # Def: by design, a pure block has the input and output dimension to be the same\n",
    "        for n in range(N_layer):\n",
    "            # Def: This is what we defined as a pure MLP block\n",
    "            # Why the Affine function?\n",
    "            #   R: this is for the Neuro-network to learn how to normalize the data between layer\n",
    "            modules.append(ResBlock(int_dim, int_dim))\n",
    "            modules.append(Supact(int_dim))\n",
    "        \n",
    "        # Def: the transformation from the internal dimension to the output dimension of the\n",
    "        #      data vector we intend to emulate\n",
    "        \n",
    "        modules.append(nn.Linear(int_dim, output_dim))\n",
    "        modules.append(Affine())\n",
    "        # NN.SEQUENTIAL is a PYTHORCH function DEFINED AT: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "        # This function stacks up layers in the modules-list in sequence to create the whole model\n",
    "        self.resmlp =nn.Sequential(*modules)#\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x is a cosmological parameter set you feed in the model\n",
    "        out = self.resmlp(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TRF(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, int_dim, N_channels):\n",
    "\n",
    "        super(TRF, self).__init__()\n",
    "\n",
    "        modules=[]\n",
    "\n",
    "        # Def: we will set the internal dimension as multiple of 128 (reason: just simplicity)\n",
    "        int_dim = int_dim * 128\n",
    "\n",
    "        # Def: we will only change the dimension of the datavector using linear transformations  \n",
    "        \n",
    "        \n",
    "        n_channels = N_channels\n",
    "        int_dim_trf = 5120\n",
    "        modules.append(nn.Linear(input_dim, int_dim))\n",
    "        modules.append(ResBlock(int_dim, int_dim))\n",
    "        modules.append(Supact(int_dim))\n",
    "        modules.append(ResBlock(int_dim, int_dim))\n",
    "        modules.append(Supact(int_dim))\n",
    "        modules.append(ResBlock(int_dim, int_dim))\n",
    "        modules.append(Supact(int_dim))\n",
    "        modules.append(nn.Linear(int_dim, int_dim_trf))\n",
    "        modules.append(Supact(int_dim_trf))\n",
    "        modules.append(Better_Attention(int_dim_trf, n_channels))\n",
    "        modules.append(Better_Transformer(int_dim_trf, n_channels))\n",
    "        #modules.append(nn.Tanh())\n",
    "        #modules.append(Better_Attention(int_dim_trf, n_channels))\n",
    "        #modules.append(Better_Transformer(int_dim_trf, n_channels))\n",
    "        #modules.append(nn.Tanh())\n",
    "        #modules.append(Better_Attention(int_dim_trf, n_channels))\n",
    "        #modules.append(Better_Transformer(int_dim_trf, n_channels))\n",
    "        #modules.append(nn.Tanh())\n",
    "        modules.append(nn.Linear(int_dim_trf, output_dim))\n",
    "        modules.append(Affine())\n",
    "\n",
    "        self.trf =nn.Sequential(*modules)#\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #x is a cosmological parameter set you feed in the model\n",
    "        \n",
    "        \n",
    "\n",
    "        out = self.trf(x)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up and load models\n",
    "model = TRF(input_dim=9,output_dim=4998,int_dim=intdim,N_channels=nc)\n",
    "\n",
    "#model=model.module.to(device)\n",
    "model=model.to(device)\n",
    "model = nn.DataParallel(model)#.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325251b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the covmat inverse\n",
    "covinv=np.load('extra/cosvarinvTT.npy',allow_pickle=True)[:camb_ell_range,:camb_ell_range]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn on the model to evaluating mode\n",
    "model.load_state_dict(torch.load(PATH+'.pt',map_location=device))\n",
    "model=model.module.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd160db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict functions\n",
    "def predict(X):\n",
    "    with torch.no_grad():\n",
    "        X_norm=((X - X_mean) / X_std)\n",
    "        X_norm[:,6:]=0\n",
    "\n",
    "        X_norm.to(device)\n",
    "    \n",
    "        pred=model(X_norm)\n",
    "        \n",
    "        M_pred=pred.to(device)\n",
    "        y_pred = (M_pred.float() *Y_std.float()+Y_mean.float()).cpu().numpy()\n",
    "        \n",
    "    return y_pred\n",
    "\n",
    "def unnormalize(y_pred,X):\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i]=y_pred[i]*(np.exp(X[i,5].cpu().float().numpy()))/(np.exp(2*X[i,3].cpu().float().numpy()))\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "testing_samples=torch.Tensor(testing_samples).to(device)\n",
    "\n",
    "testing_results=predict(testing_samples)\n",
    "\n",
    "pred=unnormalize(testing_results,testing_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5884de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing chi2\n",
    "diff=testing_data_vectors- pred\n",
    "\n",
    "loss1 = np.diag(diff@covinv@diff.T)\n",
    "\n",
    "\n",
    "print(r'mean $\\chi^2=$',np.mean(loss1))\n",
    "print(r'median $\\chi^2=$',np.median(loss1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ba523",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r'$f(\\chi^2>1)=$',len([1 for i in loss1 if i > 1])/len(loss1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454dd1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r'$f(\\chi^2>0.2)=$',len([1 for i in loss1 if i > 0.2])/len(loss1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807582b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
