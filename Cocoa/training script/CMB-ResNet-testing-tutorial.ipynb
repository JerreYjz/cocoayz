{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import camb\n",
    "import scipy.linalg\n",
    "from camb import model, initialpower\n",
    "from camb.dark_energy import DarkEnergyPPF, DarkEnergyFluid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the ell range\n",
    "camb_ell_min          = 2#30\n",
    "camb_ell_max          = 5000\n",
    "camb_ell_range        = camb_ell_max  - camb_ell_min \n",
    "\n",
    "#load test data\n",
    "testing_samples=np.load('parametersamples/cos_pkc_T128.npy',allow_pickle=True)[10000:]\n",
    "\n",
    "testing_data_vectors=np.load('datavectors/Planck128/cos_pkc_T128_TT_acc.npy',allow_pickle=True)[10000:,:4998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set device\n",
    "#I prefer using CPU for model usage after training\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#set the number of layers and internal dimension of the ResMLP blocks\n",
    "Nlayer=3\n",
    "intdim=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fbcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Supact(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(Supact, self).__init__()\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.ones(in_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(in_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        inv = (1+torch.exp(-1*torch.mul(self.beta,x))).pow_(-1)\n",
    "        fac = 1-self.gamma\n",
    "        mult = self.gamma + torch.mul(inv,fac)\n",
    "        return torch.mul(mult,x)\n",
    "\n",
    "class Affine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Affine, self).__init__()\n",
    "\n",
    "        # This function is designed for the Neuro-network to learn how to normalize the data between\n",
    "        # layers. we will initiate gains and bias both at 1 \n",
    "        self.gain = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return x * self.gain + self.bias\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        if in_size != out_size: \n",
    "            self.skip = nn.Linear(in_size, out_size, bias=False) # we don't consider this. remove?\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_size, out_size)\n",
    "        self.layer2 = nn.Linear(out_size, out_size)\n",
    "\n",
    "        self.norm1 = Affine()\n",
    "        self.norm2 = Affine()\n",
    "\n",
    "        self.act1 = Supact(in_size)#nn.Tanh()#nn.ReLU()#\n",
    "        self.act2 = Supact(in_size)#nn.Tanh()#nn.ReLU()#\n",
    "\n",
    "    def forward(self, x):\n",
    "        xskip = self.skip(x)\n",
    "\n",
    "        o1 = self.act1(self.layer1(self.norm1(x)))\n",
    "        o2 = self.act2(self.layer2(self.norm2(o1))) + xskip\n",
    "\n",
    "        return o2\n",
    "\n",
    "\n",
    "class ResMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, int_dim, N_layer):\n",
    "\n",
    "        super(ResMLP, self).__init__()\n",
    "\n",
    "        modules=[]\n",
    "\n",
    "        # Def: we will set the internal dimension as multiple of 128 (reason: just simplicity)\n",
    "        int_dim = int_dim * 128\n",
    "\n",
    "        # Def: we will only change the dimension of the datavector using linear transformations  \n",
    "        modules.append(nn.Linear(input_dim, int_dim))\n",
    "        \n",
    "        # Def: by design, a pure block has the input and output dimension to be the same\n",
    "        for n in range(N_layer):\n",
    "            # Def: This is what we defined as a pure MLP block\n",
    "            # Why the Affine function?\n",
    "            #   R: this is for the Neuro-network to learn how to normalize the data between layer\n",
    "            modules.append(ResBlock(int_dim, int_dim))\n",
    "            modules.append(Supact(int_dim))\n",
    "        \n",
    "        # Def: the transformation from the internal dimension to the output dimension of the\n",
    "        #      data vector we intend to emulate\n",
    "        \n",
    "        modules.append(nn.Linear(int_dim, output_dim))\n",
    "        modules.append(Affine())\n",
    "        # NN.SEQUENTIAL is a PYTHORCH function DEFINED AT: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "        # This function stacks up layers in the modules-list in sequence to create the whole model\n",
    "        self.resmlp =nn.Sequential(*modules)#\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x is a cosmological parameter set you feed in the model\n",
    "        out = self.resmlp(x)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up and load models\n",
    "model = ResMLP(input_dim=9,output_dim=camb_ell_range,int_dim=intdim,N_layer=Nlayer)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325251b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "covinv=np.load('cosvarinvTT.npy',allow_pickle=True)[:camb_ell_range,:camb_ell_range]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./trainedemu5000plkres/chiTTAstauresrescaleplk96i\"+str(intdim)+'l'+str(Nlayer)\n",
    "extrainfo=np.load(\"extra/extrainfo_plk_tt_third_T256_96.npy\",allow_pickle=True)\n",
    "X_mean=torch.Tensor(extrainfo.item()['X_mean']).to(device)\n",
    "X_std=torch.Tensor(extrainfo.item()['X_std']).to(device)\n",
    "Y_mean=torch.Tensor(extrainfo.item()['Y_mean']).to(device)\n",
    "Y_std=torch.Tensor(extrainfo.item()['Y_std']).to(device)\n",
    "Y_mean2=torch.Tensor(extrainfo.item()['Y_mean2']).to(device)\n",
    "Y_std2=torch.Tensor(extrainfo.item()['Y_std2']).to(device)\n",
    "model.load_state_dict(torch.load(PATH+'.pt',map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd160db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    with torch.no_grad():\n",
    "        X_norm=((X.float() - X_mean.float()) / X_std.float())\n",
    "        X_norm[:,6:]=0\n",
    "        #print(X_norm)\n",
    "        pred=model(X_norm)\n",
    "        #print(pred.dtype)\n",
    "        \n",
    "        M_pred=pred.to(device)\n",
    "        y_pred = (M_pred.float() *Y_std2.float()+Y_mean2.float()).numpy()\n",
    "        \n",
    "    return y_pred\n",
    "transform_matrix=np.load('extra/PCAmat_T256_tt_third.npy',allow_pickle=True)#PCA matrix loaded\n",
    "def unnormalize(y_pred,X):\n",
    "    y_pred=np.dot(d,transform_matrix)*Y_std.cpu().float().numpy()+Y_mean.cpu().float().numpy()\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i]=y_pred[i]*(np.exp(X[i,5].cpu().float().numpy()))/(np.exp(2*X[i,3].cpu().float().numpy()))\n",
    "    return y_pred\n",
    "\n",
    "#just a small trial to see if the pipline works\n",
    "testing_samples=torch.Tensor(testing_samples)\n",
    "\n",
    "testing_results=predict(testing_samples)\n",
    "\n",
    "pred=unnormalize(testing_results,testing_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5884de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing chi2\n",
    "diff=testing_data_vectors- pred\n",
    "\n",
    "loss1 = np.diag(diff@covinv@diff.T)\n",
    "\n",
    "\n",
    "print(r'mean $\\chi^2=$',np.mean(loss1))\n",
    "print(r'median $\\chi^2=$',np.median(loss1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4027dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r'$f(\\chi^2>1)=$',len([1 for i in loss1 if i > 1])/len(loss1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a40bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r'$f(\\chi^2>0.2)=$',len([1 for i in loss1 if i > 0.2])/len(loss1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2af91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
